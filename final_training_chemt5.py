# -*- coding: utf-8 -*-
"""final training chemt5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o-TvqDjRSHwowB7CKdEir82NnYVJInZW
"""

# Step 1: Install dependencies
!pip install transformers datasets scikit-learn evaluate -q
!pip install rouge_score


# Step 2: Import libraries
import os
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import numpy as np
import evaluate

# ðŸš« Disable Weights & Biases
#os.environ["WANDB_DISABLED"] = "true"

# Step 3: Load datasets
train_df = pd.read_csv("rxn_train.csv")
val_df = pd.read_csv("rxn_val.csv")
test_df = pd.read_csv("rxn_test.csv")

# Step 4: Preprocess: separate input/output
train_df = train_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})
val_df = val_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})
test_df = test_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})

# Split 'reactants>reagents>product' into input/output
def split_input_output(df):
    splits = df["full"].str.split(">", expand=True)
    df["input_text"] = splits[0] + ">" + splits[1]  # reactants>reagents
    df["target_text"] = splits[2]                   # product
    return df

train_df = split_input_output(train_df)
val_df = split_input_output(val_df)
test_df = split_input_output(test_df)

# Step 5: Convert to HuggingFace Datasets
train_dataset = Dataset.from_pandas(train_df[["input_text", "target_text"]])
val_dataset = Dataset.from_pandas(val_df[["input_text", "target_text"]])
test_dataset = Dataset.from_pandas(test_df[["input_text", "target_text"]])

# Step 6: Load tokenizer and model (T5-small)
model_checkpoint = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

# Step 7: Tokenize data
max_input_length = 128
max_target_length = 64

def preprocess_function(examples):
    inputs = tokenizer(examples["input_text"], max_length=max_input_length, padding="max_length", truncation=True)
    targets = tokenizer(examples["target_text"], max_length=max_target_length, padding="max_length", truncation=True)

    inputs["labels"] = targets["input_ids"]
    return inputs

train_dataset = train_dataset.map(preprocess_function, batched=True)
val_dataset = val_dataset.map(preprocess_function, batched=True)
test_dataset = test_dataset.map(preprocess_function, batched=True)

# Step 8: Define TrainingArguments
training_args = TrainingArguments(
    output_dir="./t5-chem-product-generator",
    #evaluation_strategy="epoch",
    learning_rate=3e-4,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=5,
    #predict_with_generate=True,
    logging_dir="./logs",
    report_to="none"  # ðŸ‘ˆ disables Weights & Biases cleanly
)


# Step 9: Define metric
rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    return rouge.compute(predictions=decoded_preds, references=decoded_labels)

# Step 10: Initialize Trainer
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Step 11: Train the model
trainer.train()

# Step 12: Generate predictions on test set
test_results = trainer.predict(test_dataset)
decoded_preds = tokenizer.batch_decode(test_results.predictions, skip_special_tokens=True)

# Display sample predictions
for i in range(5):
    print(f"Input: {test_df['input_text'][i]}")
    print(f"Expected Output: {test_df['target_text'][i]}")
    print(f"Predicted Output: {decoded_preds[i]}")
    print("-" * 60)

import torch

import torch
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

batch_size = 8  # reduce if still OOM
decoded_preds = []

test_inputs_list = test_df["input_text"].tolist()

for i in tqdm(range(0, len(test_inputs_list), batch_size)):
    batch_inputs = test_inputs_list[i:i+batch_size]

    # Tokenize batch
    inputs = tokenizer(
        batch_inputs,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=max_input_length
    )

    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_target_length,
            num_beams=4,
            early_stopping=True
        )

    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    decoded_preds.extend(preds)

# Display some results
for i in range(5):
    print(f"Input: {test_df['input_text'][i]}")
    print(f"Expected Output: {test_df['target_text'][i]}")
    print(f"Predicted Output: {decoded_preds[i]}")
    print("-" * 60)

# Save all predictions
test_df["predicted_output"] = decoded_preds
test_df.to_csv("t5_predictions.csv", index=False)

# Step 1: Install dependencies
!pip install transformers datasets scikit-learn evaluate -q
!pip install rouge_score

# Step 2: Import libraries
import os
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import numpy as np
import evaluate

# Step 3: Load datasets
train_df = pd.read_csv("rxn_train.csv")
val_df = pd.read_csv("rxn_val.csv")
test_df = pd.read_csv("rxn_test.csv")

# Step 4: Preprocess: separate input/output
train_df = train_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})
val_df = val_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})
test_df = test_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})

def split_input_output(df):
    splits = df["full"].str.split(">", expand=True)
    df["input_text"] = splits[0] + ">" + splits[1]  # reactants>reagents
    df["target_text"] = splits[2]                   # product
    return df

train_df = split_input_output(train_df)
val_df = split_input_output(val_df)
test_df = split_input_output(test_df)

# Step 5: Convert to HuggingFace Datasets
train_dataset = Dataset.from_pandas(train_df[["input_text", "target_text"]])
val_dataset = Dataset.from_pandas(val_df[["input_text", "target_text"]])
test_dataset = Dataset.from_pandas(test_df[["input_text", "target_text"]])

# Step 6: Load tokenizer and model
model_checkpoint = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

# Step 7: Tokenize data
max_input_length = 128
max_target_length = 64

def preprocess_function(examples):
    inputs = tokenizer(examples["input_text"], max_length=max_input_length, padding="max_length", truncation=True)
    targets = tokenizer(examples["target_text"], max_length=max_target_length, padding="max_length", truncation=True)
    inputs["labels"] = targets["input_ids"]
    return inputs

train_dataset = train_dataset.map(preprocess_function, batched=True)
val_dataset = val_dataset.map(preprocess_function, batched=True)
test_dataset = test_dataset.map(preprocess_function, batched=True)

# Step 8: Define TrainingArguments
training_args = TrainingArguments(
    output_dir="./t5-chem-product-generator",
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=20,
    logging_dir="./logs",
    report_to="none"
)

# Step 9: Define metric
rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    return rouge.compute(predictions=decoded_preds, references=decoded_labels)

# Step 10: Initialize Trainer
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Step 11: Train the model
trainer.train()

# Step 12: Manual prediction loop with beam search settings
import torch
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

batch_size = 8
decoded_preds = []

test_inputs_list = test_df["input_text"].tolist()

for i in tqdm(range(0, len(test_inputs_list), batch_size)):
    batch_inputs = test_inputs_list[i:i+batch_size]

    # Tokenize batch
    inputs = tokenizer(
        batch_inputs,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=max_input_length
    )

    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_target_length,
            num_beams=8,          # more beams for better search
            length_penalty=1.0,
            early_stopping=True
        )

    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    decoded_preds.extend(preds)

# Step 13: Display sample predictions
for i in range(5):
    print(f"Input: {test_df['input_text'][i]}")
    print(f"Expected Output: {test_df['target_text'][i]}")
    print(f"Predicted Output: {decoded_preds[i]}")
    print("-" * 60)

# Step 14: Save predictions
test_df["predicted_output"] = decoded_preds
test_df.to_csv("t5_predictions.csv", index=False)

# Step 1: Install dependencies
!pip install transformers datasets scikit-learn evaluate python-Levenshtein -q
!pip install rouge_score

# Step 2: Import libraries
import os
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import numpy as np
import evaluate
import Levenshtein
import torch
from tqdm import tqdm

# Step 3: Load datasets
train_df = pd.read_csv("rxn_train.csv")
val_df = pd.read_csv("rxn_val.csv")
test_df = pd.read_csv("rxn_test.csv")

# Step 4: Preprocess: separate input/output
train_df = train_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})
val_df = val_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})
test_df = test_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})

def split_input_output(df):
    splits = df["full"].str.split(">", expand=True)
    df["input_text"] = splits[0] + ">" + splits[1]  # reactants>reagents
    df["target_text"] = splits[2]                   # product
    return df

train_df = split_input_output(train_df)
val_df = split_input_output(val_df)
test_df = split_input_output(test_df)

# Step 5: Convert to HuggingFace Datasets
train_dataset = Dataset.from_pandas(train_df[["input_text", "target_text"]])
val_dataset = Dataset.from_pandas(val_df[["input_text", "target_text"]])
test_dataset = Dataset.from_pandas(test_df[["input_text", "target_text"]])

# Step 6: Load tokenizer and model
model_checkpoint = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

# Step 7: Tokenize data
max_input_length = 128
max_target_length = 64

def preprocess_function(examples):
    model_inputs = tokenizer(examples["input_text"], max_length=max_input_length, padding="max_length", truncation=True)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["target_text"], max_length=max_target_length, padding="max_length", truncation=True)

    # Replace pad token id's with -100 so they're ignored by loss
    labels_ids = labels["input_ids"]
    labels_ids = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels_ids]
    model_inputs["labels"] = labels_ids
    return model_inputs

train_dataset = train_dataset.map(preprocess_function, batched=True)
val_dataset = val_dataset.map(preprocess_function, batched=True)
test_dataset = test_dataset.map(preprocess_function, batched=True)

# Step 8: Define TrainingArguments
training_args = TrainingArguments(
    output_dir="./t5-chem-product-generator",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=20,
    predict_with_generate=True,
    logging_dir="./logs",
    report_to="none"
)

# Step 9: Define metric with Levenshtein similarity
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Compute Levenshtein similarity for each prediction
    levenshtein_scores = [
        Levenshtein.ratio(pred.strip(), ref.strip()) for pred, ref in zip(decoded_preds, decoded_labels)
    ]
    avg_levenshtein = np.mean(levenshtein_scores)

    # Also return ROUGE-L for reference
    rouge = evaluate.load("rouge")
    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)

    rouge_result["levenshtein_similarity"] = avg_levenshtein
    return rouge_result

# Step 10: Initialize Trainer
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Step 11: Train the model
trainer.train()

# Step 12: Generate predictions on test set
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

batch_size = 8
decoded_preds = []
test_inputs_list = test_df["input_text"].tolist()

for i in tqdm(range(0, len(test_inputs_list), batch_size)):
    batch_inputs = test_inputs_list[i:i+batch_size]
    inputs = tokenizer(batch_inputs, return_tensors="pt", padding="max_length", truncation=True, max_length=max_input_length)
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_target_length,
            num_beams=8,
            length_penalty=1.0,
            early_stopping=True
        )

    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    decoded_preds.extend(preds)

# Step 13: Save and display results
test_df["predicted_output"] = decoded_preds
test_df["levenshtein_similarity"] = [
    Levenshtein.ratio(pred.strip(), ref.strip()) for pred, ref in zip(decoded_preds, test_df["target_text"])
]
test_df.to_csv("t5_predictions.csv", index=False)

for i in range(5):
    print(f"Input: {test_df['input_text'][i]}")
    print(f"Expected Output: {test_df['target_text'][i]}")
    print(f"Predicted Output: {decoded_preds[i]}")
    print(f"Levenshtein Similarity: {test_df['levenshtein_similarity'][i]:.4f}")
    print("-" * 60)

# Step 1: Install dependencies
!pip install transformers datasets scikit-learn evaluate -q
!pip install rouge_score

# Step 2: Import libraries
import os
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import numpy as np
import evaluate

# Step 3: Load datasets
train_df = pd.read_csv("rxn_train.csv")
val_df = pd.read_csv("rxn_val.csv")
test_df = pd.read_csv("rxn_test.csv")

# Step 4: Preprocess: separate input/output
train_df = train_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})
val_df = val_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})
test_df = test_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})

def split_input_output(df):
    splits = df["full"].str.split(">", expand=True)
    df["input_text"] = splits[0] + ">" + splits[1]  # reactants>reagents
    df["target_text"] = splits[2]                   # product
    return df

train_df = split_input_output(train_df)
val_df = split_input_output(val_df)
test_df = split_input_output(test_df)

# Step 5: Convert to HuggingFace Datasets
train_dataset = Dataset.from_pandas(train_df[["input_text", "target_text"]])
val_dataset = Dataset.from_pandas(val_df[["input_text", "target_text"]])
test_dataset = Dataset.from_pandas(test_df[["input_text", "target_text"]])

# Step 6: Load tokenizer and model
model_checkpoint = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

# Step 7: Tokenize data
max_input_length = 128
max_target_length = 256

def preprocess_function(examples):
    inputs = ["predict product: " + ex for ex in examples["input_text"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, padding="max_length", truncation=True)

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["target_text"], max_length=max_target_length, padding="max_length", truncation=True)
    labels["input_ids"] = [
        [(token if token != tokenizer.pad_token_id else -100) for token in label]
        for label in labels["input_ids"]
    ]
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


train_dataset = train_dataset.map(preprocess_function, batched=True)
val_dataset = val_dataset.map(preprocess_function, batched=True)
test_dataset = test_dataset.map(preprocess_function, batched=True)

# Step 8: Define TrainingArguments
training_args = TrainingArguments(
    output_dir="./t5-chem-product-generator",
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=1,
    logging_dir="./logs",
    report_to="none"
)

# Step 9: Define metric
rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    return rouge.compute(predictions=decoded_preds, references=decoded_labels)

# Step 10: Initialize Trainer
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Step 11: Train the model
trainer.train()

# Step 12: Manual prediction loop with beam search settings
import torch
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

batch_size = 8
decoded_preds = []

test_inputs_list = test_df["input_text"].tolist()

for i in tqdm(range(0, len(test_inputs_list), batch_size)):
    batch_inputs = test_inputs_list[i:i+batch_size]

    # Tokenize batch
    inputs = tokenizer(
        batch_inputs,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=max_input_length
    )

    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_target_length,
            num_beams=12,          # more beams for better search
            length_penalty=1.0,

            #early_stopping=True
        )

    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    decoded_preds.extend(preds)

# Step 13: Display sample predictions
for i in range(5):
    print(f"Input: {test_df['input_text'][i]}")
    print(f"Expected Output: {test_df['target_text'][i]}")
    print(f"Predicted Output: {decoded_preds[i]}")
    print("-" * 60)

# Step 14: Save predictions
test_df["predicted_output"] = decoded_preds
test_df.to_csv("t5_predictions.csv", index=False)

# Step 12: Manual prediction loop with beam search settings
import torch
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

batch_size = 8
decoded_preds = []

test_inputs_list = test_df["input_text"].tolist()

for i in tqdm(range(0, len(test_inputs_list), batch_size)):
    batch_inputs = test_inputs_list[i:i+batch_size]

    # Tokenize batch
    inputs = tokenizer(
        batch_inputs,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=max_input_length
    )

    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_target_length,
            num_beams=12,          # more beams for better search
            length_penalty=1.0,

            #early_stopping=True
        )

    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    decoded_preds.extend(preds)

# Step 13: Display sample predictions
for i in range(5):
    print(f"Input: {test_df['input_text'][i]}")
    print(f"Expected Output: {test_df['target_text'][i]}")
    print(f"Predicted Output: {decoded_preds[i]}")
    print("-" * 60)

# Step 14: Save predictions
test_df["predicted_output"] = decoded_preds
test_df.to_csv("t5_predictions.csv", index=False)

# Step 1: Install dependencies
!pip install transformers datasets scikit-learn evaluate -q
!pip install rouge_score

# Step 2: Import libraries
import os
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import numpy as np
import evaluate

# Step 3: Load datasets
train_df = pd.read_csv("rxn_train.csv")
val_df = pd.read_csv("rxn_val.csv")
test_df = pd.read_csv("rxn_test.csv")

# Step 4: Preprocess: separate input/output
train_df = train_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})
val_df = val_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})
test_df = test_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})

def split_input_output(df):
    splits = df["full"].str.split(">", expand=True)
    df["input_text"] = splits[0] + ">" + splits[1]  # reactants>reagents
    df["target_text"] = splits[2]                   # product
    return df

train_df = split_input_output(train_df)
val_df = split_input_output(val_df)
test_df = split_input_output(test_df)

# Step 5: Convert to HuggingFace Datasets
train_dataset = Dataset.from_pandas(train_df[["input_text", "target_text"]])
val_dataset = Dataset.from_pandas(val_df[["input_text", "target_text"]])
test_dataset = Dataset.from_pandas(test_df[["input_text", "target_text"]])

# Step 6: Load tokenizer and model
model_checkpoint = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

# Step 7: Tokenize data
max_input_length = 256
max_target_length = 512

def preprocess_function(examples):
    inputs = ["predict product: " + ex for ex in examples["input_text"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, padding="max_length", truncation=True)

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["target_text"], max_length=max_target_length, padding="max_length", truncation=True)
    labels["input_ids"] = [
        [(token if token != tokenizer.pad_token_id else -100) for token in label]
        for label in labels["input_ids"]
    ]
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


train_dataset = train_dataset.map(preprocess_function, batched=True)
val_dataset = val_dataset.map(preprocess_function, batched=True)
test_dataset = test_dataset.map(preprocess_function, batched=True)

# Step 8: Define TrainingArguments
training_args = TrainingArguments(
    output_dir="./t5-chem-product-generator",
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=10,
    logging_dir="./logs",
    report_to="none"
)

# Step 9: Define metric
rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    return rouge.compute(predictions=decoded_preds, references=decoded_labels)

# Step 10: Initialize Trainer
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Step 11: Train the model
trainer.train()

# Step 12: Manual prediction loop with beam search settings
import torch
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

batch_size = 8
decoded_preds = []

test_inputs_list = test_df["input_text"].tolist()

for i in tqdm(range(0, len(test_inputs_list), batch_size)):
    batch_inputs = test_inputs_list[i:i+batch_size]

    # Tokenize batch
    inputs = tokenizer(
        batch_inputs,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=max_input_length
    )

    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_target_length,
            num_beams=12,          # more beams for better search
            length_penalty=1.0,

            #early_stopping=True
        )

    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    decoded_preds.extend(preds)

# Step 13: Display sample predictions
for i in range(5):
    print(f"Input: {test_df['input_text'][i]}")
    print(f"Expected Output: {test_df['target_text'][i]}")
    print(f"Predicted Output: {decoded_preds[i]}")
    print("-" * 60)

# Step 14: Save predictions
test_df["predicted_output"] = decoded_preds
test_df.to_csv("t5_predictions.csv", index=False)

# Step 1: Install dependencies
!pip install transformers datasets scikit-learn evaluate -q
!pip install rouge_score

# Step 2: Import libraries
import os
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import numpy as np
import evaluate

# Step 3: Load datasets
train_df = pd.read_csv("rxn_train.csv")
val_df = pd.read_csv("rxn_val.csv")
test_df = pd.read_csv("rxn_test.csv")

# Step 4: Preprocess: separate input/output
train_df = train_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})
val_df = val_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})
test_df = test_df.rename(columns={"reactants>reagents>production": "full", "class": "label"})

def split_input_output(df):
    splits = df["full"].str.split(">", expand=True)
    df["input_text"] = splits[0] + ">" + splits[1]  # reactants>reagents
    df["target_text"] = splits[2]                   # product
    return df

train_df = split_input_output(train_df)
val_df = split_input_output(val_df)
test_df = split_input_output(test_df)

# Step 5: Convert to HuggingFace Datasets
train_dataset = Dataset.from_pandas(train_df[["input_text", "target_text"]])
val_dataset = Dataset.from_pandas(val_df[["input_text", "target_text"]])
test_dataset = Dataset.from_pandas(test_df[["input_text", "target_text"]])

# Step 6: Load tokenizer and model
model_checkpoint = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

# Step 7: Tokenize data
max_input_length = 256
max_target_length = 512

def preprocess_function(examples):
    inputs = ["predict product: " + ex for ex in examples["input_text"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, padding="max_length", truncation=True)

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["target_text"], max_length=max_target_length, padding="max_length", truncation=True)
    labels["input_ids"] = [
        [(token if token != tokenizer.pad_token_id else -100) for token in label]
        for label in labels["input_ids"]
    ]
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


train_dataset = train_dataset.map(preprocess_function, batched=True)
val_dataset = val_dataset.map(preprocess_function, batched=True)
test_dataset = test_dataset.map(preprocess_function, batched=True)

# Step 8: Define TrainingArguments
training_args = TrainingArguments(
    output_dir="./t5-chem-product-generator",
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=5,
    logging_dir="./logs",
    report_to="none"
)

# Step 9: Define metric
rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    return rouge.compute(predictions=decoded_preds, references=decoded_labels)

# Step 10: Initialize Trainer
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Step 11: Train the model
trainer.train()

# Save model and tokenizer
'''trainer.save_model("./t5-chem-product-generator")
tokenizer.save_pretrained("./t5-chem-product-generator")

from google.colab import files
import shutil

# Zip the model folder
shutil.make_archive("t5-chem-product-generator", 'zip', "./t5-chem-product-generator")

# Download the zip file
files.download("t5-chem-product-generator.zip")

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load model from local folder (unzipped)
model = AutoModelForSeq2SeqLM.from_pretrained("./t5-chem-product-generator")
tokenizer = AutoTokenizer.from_pretrained("./t5-chem-product-generator")

# Move to device
import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)


# Step 12: Manual prediction loop with beam search settings
import torch
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

batch_size = 8
decoded_preds = []

test_inputs_list = test_df["input_text"].tolist()

for i in tqdm(range(0, len(test_inputs_list), batch_size)):
    batch_inputs = test_inputs_list[i:i+batch_size]

    # Tokenize batch
    inputs = tokenizer(
        batch_inputs,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=max_input_length
    )

    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_target_length,
            num_beams=12,          # more beams for better search
            length_penalty=1.0,

            #early_stopping=True
        )

    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    decoded_preds.extend(preds)

# Step 13: Display sample predictions
for i in range(5):
    print(f"Input: {test_df['input_text'][i]}")
    print(f"Expected Output: {test_df['target_text'][i]}")
    print(f"Predicted Output: {decoded_preds[i]}")
    print("-" * 60)

# Step 14: Save predictions
test_df["predicted_output"] = decoded_preds
test_df.to_csv("t5_predictions.csv", index=False)'''

# Save model and tokenizer
trainer.save_model("./t5-chem-product-generator")
tokenizer.save_pretrained("./t5-chem-product-generator")

from google.colab import files
import shutil

# Zip the model folder
shutil.make_archive("t5-chem-product-generator", 'zip', "./t5-chem-product-generator")

# Download the zip file
files.download("t5-chem-product-generator.zip")

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load model from local folder (unzipped)
model = AutoModelForSeq2SeqLM.from_pretrained("./t5-chem-product-generator")
tokenizer = AutoTokenizer.from_pretrained("./t5-chem-product-generator")

# Move to device
import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)



import zipfile

zip_file_path = "/content/t5-chem-product-generator.zip"
extract_path = "/content/t5-chem-product-generator"

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
      zip_ref.extractall(extract_path)

# ==========================
# Prediction Script
# ==========================

import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm

# Load fine-tuned model & tokenizer
model_path = "./t5-chem-product-generator"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Load test data
test_df = pd.read_csv("rxn_test.csv")

# Ensure consistent column naming
test_df = test_df.rename(columns={"reactants>reagents>production": "full"})
splits = test_df["full"].str.split(">", expand=True)
test_df["input_text"] = splits[0] + ">" + splits[1]   # reactants>reagents
test_df["target_text"] = splits[2]                    # product

# Prediction parameters
batch_size = 8
max_input_length = 256
max_target_length = 512

decoded_preds = []

# Loop through test set in batches
for i in tqdm(range(0, len(test_df), batch_size)):
    batch_inputs = test_df["input_text"].tolist()[i:i+batch_size]

    # Tokenize
    inputs = tokenizer(
        ["predict product: " + text for text in batch_inputs],
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=max_input_length
    )

    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    # Generate predictions
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_target_length,
            num_beams=12,         # beam search
            length_penalty=1.0
        )

    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    decoded_preds.extend(preds)

# Add predictions to DataFrame
test_df["predicted_output"] = decoded_preds

# Show sample results
for i in range(5):
    print(f"Input: {test_df['input_text'][i]}")
    print(f"Expected: {test_df['target_text'][i]}")
    print(f"Predicted: {test_df['predicted_output'][i]}")
    print("-" * 60)

# Save predictions
test_df.to_csv("t5_predictions.csv", index=False)
print("âœ… Predictions saved to t5_predictions.csv")